import { OpenAI } from 'openai/client.js';
import { LLM } from './llm.interface';

class OpenAILLM implements LLM {
	private _client: OpenAI | null = null;

	get client(): OpenAI {
		if (!this._client) {
			this._client = new OpenAI({
				apiKey: process.env.OPENAI_API_KEY
			});
		}
		return this._client!;
	}

	async generateText(prompt: string): Promise<string> {
		const completion = await this.client.chat.completions.create({
			model: process.env.OPENAI_TEXT_MODEL || 'gpt-3.5-turbo',
			messages: [
				{
					role: 'user',
					content: prompt
				}
			],
			max_completion_tokens: 2000
		});

		const responseContent = completion.choices[0]?.message?.content;

		if (!responseContent) {
			throw new Error('No response content received from OpenAI');
		}

		return responseContent;
	}

	async generateImage(prompt: string): Promise<string> {
		const response = await this.client.images.generate({
			model: process.env.OPENAI_IMAGE_MODEL || 'dall-e-3',
			prompt,
			n: 1,
			size: '1024x1024'
		});

		if (!response.data || response.data.length === 0 || !response.data[0].url) {
			throw new Error('No image generated by OpenAI');
		}
		return response.data?.[0].url;
	}
}

export const openaiLLM = new OpenAILLM();
